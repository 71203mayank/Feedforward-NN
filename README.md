# Feedforward Neural Network from Scratch ‚Äì Exploring Batch Normalization, Initialization Techniques & Activation Functions
This repository contains a custom implementation of a Feedforward Neural Network (FNN) from scratch to study the impact of Batch Normalization (BN), different weight initialization techniques (Xavier, He, etc.), and various activation functions on model performance.

## üîç Key Features:
1. **MNIST Dataset**: Trains the model on the handwritten digits dataset.
2. **Batch Normalization (BN)**: Applied at different layers to analyze its effect on training speed and performance.
3. **Weight Initialization**: Experiments with Xavier, He, and random initialization to observe convergence behavior.
4. **Activation Functions**: Includes ReLU, Sigmoid, and Tanh to compare their influence on training.
5. **Performance Analysis**: Tracks accuracy, loss curves, and convergence rates for different settings.

This project aims to provide insights into how architectural and training choices affect deep learning models, helping to build an intuitive understanding of neural networks. üöÄ

### üîó Feel free to explore, experiment, and contribute!
